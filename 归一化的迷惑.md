归一化的迷惑
---
归一化可以被误解成Standardization, Normalization 或regularization。
其中文的准确翻译分别是：标准化，正态化，正则化。

#### Standardization 标准化 ####

在数据预处理中，Scalar量化是非常重要的步骤，尤其是对SVM这样难以汇聚的算法，标准化和正态化几乎是必须的。因为不同度量的特征难以可视化，而且预测的表现不佳。还会使梯度下降算法变慢甚至难以汇聚。基于矩阵及梯度的预测通常假定特征是Standardized数据。决策树是个例外，对任何度量的数据都很健壮。

以scikit-learn为例，提供了几种标准化的方法：
* StandardScaler：将数据量化为单位方差,均值为零
* MinMaxScaler：将数据量化为[0,1]范围之间
* MaxAbsScaler：正数等同于MinMaxScaler，负数也同样投射至[0,1]
* RobustScaler:基于percentiles量化，不会受少量边际噪点影响
* QuantileTransformer(uniform output)：将数据转换至均匀分布，这样噪点将和正常数据混合。
* QuantileTransformer (Gaussian output)：将数据转换成高斯分布

其中，RobustScaler和QuantileTransformer是健壮的，因为移除噪点不会影响量化的结果。
还有一种常见的变换是log变换，当特征的峰度为正时，log变换通常有效，且不会丢失信息。

#### Normalization 正态化 ####

正态化的过程是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用。该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础.
Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。
这样做是因为当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。

Normalizer和StandardScaler的最大不同是，
**Normalizer作用于行，StandardScaler作用于列。**

#### Regularization 正则化 ####
机器学习模型使用规则化系数来抑制过拟合，这种对目标方程增加p-范数的行为通常叫做正则化。

* L0范数：向量中非零元素的个数
如果用L0规则化一个参数矩阵W，就是希望W中大部分元素是零，实现稀疏。
* L1范数：向量中各个元素的绝对值之和，也叫”系数规则算子（Lasso regularization）“
L1范数也可以实现稀疏，通过将无用特征对应的参数W置为零实现。

L0和L1都可以实现稀疏化，不过一般选用L1而不用L0，原因包括：1）L0范数很难优化求解（NP难）；2）L1是L0的最优凸近似，比L0更容易优化求解。
* L2范数：向量各元素的平方和然后开方，用在回归模型中也称为岭回归（Ridge regression）。
L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。
L2范数除了避免过拟合问题，还有一个优点是有助于处理condition number不好的情况下，矩阵求解困难的问题。condition number是对系统ill-condition程度的一个衡量标准。假设有方程组Ax=b，需要求解x。如果A或者b发生轻微改变，就会使得x的解发生很大变化，那么这个方程组系统就是ill-condition的，反之就是well-condition的。
